{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import cuda\n",
    "from chainer import Variable\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from chainer import cuda\n",
    "from chainer.functions.pooling import pooling_2d\n",
    "from chainer.utils import conv\n",
    "\n",
    "if cuda.cudnn_enabled:\n",
    "    cudnn = cuda.cudnn\n",
    "    libcudnn = cudnn.cudnn\n",
    "\n",
    "\n",
    "class MaxPooling2D(pooling_2d.Pooling2D):\n",
    "\n",
    "    \"\"\"Max pooling over a set of 2d planes.\"\"\"\n",
    "\n",
    "    def forward_cpu(self, x):\n",
    "        n, c, h, w = x[0].shape\n",
    "        col = conv.im2col_cpu(\n",
    "            x[0], self.kh, self.kw, self.sy, self.sx, self.ph, self.pw,\n",
    "            pval=-float('inf'), cover_all=self.cover_all)\n",
    "        n, c, kh, kw, out_h, out_w = col.shape\n",
    "        col = col.reshape(n, c, kh * kw, out_h, out_w)\n",
    "\n",
    "        # We select maximum twice, since the implementation using numpy.choose\n",
    "        # hits its bug when kh * kw >= 32.\n",
    "        self.indexes = col.argmax(axis=2)\n",
    "        y = col.max(axis=2)\n",
    "        return y, self.indexes, np.array([h, w])\n",
    "\n",
    "    def forward_gpu(self, x):\n",
    "        if (cuda.cudnn_enabled and self.use_cudnn and\n",
    "                pooling_2d._check_cudnn_acceptable_type(x[0].dtype)):\n",
    "            return super(MaxPooling2D, self).forward_gpu(x)\n",
    "\n",
    "        n, c, h, w = x[0].shape\n",
    "        y_h = conv.get_conv_outsize(\n",
    "            h, self.kh, self.sy, self.ph, self.cover_all)\n",
    "        y_w = conv.get_conv_outsize(\n",
    "            w, self.kw, self.sx, self.pw, self.cover_all)\n",
    "        y = cuda.cupy.empty((n, c, y_h, y_w), dtype=x[0].dtype)\n",
    "        self.indexes = cuda.cupy.empty((n, c, y_h, y_w), dtype=numpy.int32)\n",
    "\n",
    "        cuda.elementwise(\n",
    "            'raw T in, int32 h, int32 w, int32 out_h, int32 out_w,'\n",
    "            'int32 kh, int32 kw, int32 sy, int32 sx, int32 ph, int32 pw',\n",
    "            'T out, S indexes',\n",
    "            '''\n",
    "               int c0    = i / (out_h * out_w);\n",
    "               int out_y = i / out_w % out_h;\n",
    "               int out_x = i % out_w;\n",
    "               int in_y_0 = max(0, out_y * sy - ph);\n",
    "               int in_y_1 = min(h, out_y * sy + kh - ph);\n",
    "               int in_x_0 = max(0, out_x * sx - pw);\n",
    "               int in_x_1 = min(w, out_x * sx + kw - pw);\n",
    "\n",
    "               T maxval = in[in_x_0 + w * (in_y_0 + h * c0)];\n",
    "               int argmax_y = in_y_0;\n",
    "               int argmax_x = in_x_0;\n",
    "               for (int y = in_y_0; y < in_y_1; ++y) {\n",
    "                 int offset_y = w * (y + h * c0);\n",
    "                 for (int x = in_x_0; x < in_x_1; ++x) {\n",
    "                   float v = in[x + offset_y];\n",
    "                   if (maxval < v) {\n",
    "                     maxval   = v;\n",
    "                     argmax_y = y;\n",
    "                     argmax_x = x;\n",
    "                   }\n",
    "                 }\n",
    "               }\n",
    "               out = maxval;\n",
    "\n",
    "               int argmax_ky = argmax_y + ph - out_y * sy;\n",
    "               int argmax_kx = argmax_x + pw - out_x * sx;\n",
    "               indexes = argmax_kx + kw * argmax_ky;\n",
    "            ''', 'max_pool_fwd')(x[0].reduced_view(),\n",
    "                                 h, w, y_h, y_w, self.kh, self.kw,\n",
    "                                 self.sy, self.sx, self.ph, self.pw,\n",
    "                                 y, self.indexes)\n",
    "        return y, self.indexes, np.array([h, w]),\n",
    "\n",
    "    def backward_cpu(self, x, gy):\n",
    "        n, c, out_h, out_w = gy[0].shape\n",
    "        h, w = x[0].shape[2:]\n",
    "        gcol = numpy.zeros(\n",
    "            (n, c, self.kh, self.kw, out_h, out_w), dtype=x[0].dtype)\n",
    "\n",
    "        # TODO(beam2d): Make it fast\n",
    "        gcol_r = numpy.rollaxis(gcol.reshape(n, c, -1, out_h, out_w), 2)\n",
    "        for i in numpy.ndindex(n, c, out_h, out_w):\n",
    "            gcol_r[self.indexes[i]][i] = gy[0][i]\n",
    "\n",
    "        gx = conv.col2im_cpu(gcol, self.sy, self.sx, self.ph, self.pw, h, w)\n",
    "        return gx,\n",
    "\n",
    "    def backward_gpu(self, x, gy):\n",
    "        if (cuda.cudnn_enabled and self.use_cudnn and\n",
    "                pooling_2d._check_cudnn_acceptable_type(x[0].dtype)):\n",
    "            return super(MaxPooling2D, self).backward_gpu(x, gy)\n",
    "\n",
    "        n, c, h, w = x[0].shape\n",
    "        y_h, y_w = gy[0].shape[2:]\n",
    "        gx = cuda.cupy.empty_like(x[0])\n",
    "\n",
    "        cuda.elementwise(\n",
    "            'raw T gy, raw S indexes, int32 h, int32 w,'\n",
    "            'int32 out_h, int32 out_w, int32 kh, int32 kw,'\n",
    "            'int32 sy, int32 sx, int32 ph, int32 pw',\n",
    "            'T gx',\n",
    "            '''\n",
    "               int c0 = i / (h * w);\n",
    "               int y  = i / w % h + ph;\n",
    "               int x  = i % w + pw;\n",
    "               int out_y_0 = max(0,     (y - kh + sy) / sy);\n",
    "               int out_y_1 = min(out_h, (y      + sy) / sy);\n",
    "               int out_x_0 = max(0,     (x - kw + sx) / sx);\n",
    "               int out_x_1 = min(out_w, (x      + sx) / sx);\n",
    "\n",
    "               T val = 0;\n",
    "               for (int out_y = out_y_0; out_y < out_y_1; ++out_y) {\n",
    "                 int ky = y - out_y * sy;\n",
    "                 for (int out_x = out_x_0; out_x < out_x_1; ++out_x) {\n",
    "                   int kx = x - out_x * sx;\n",
    "                   int offset = out_x + out_w * (out_y + out_h * c0);\n",
    "                   if (indexes[offset] == kx + kw * ky) {\n",
    "                     val = val + gy[offset];\n",
    "                   }\n",
    "                 }\n",
    "               }\n",
    "               gx = val;\n",
    "            ''',\n",
    "            'max_pool_bwd')(gy[0].reduced_view(), self.indexes.reduced_view(),\n",
    "                            h, w, y_h, y_w, self.kh, self.kw,\n",
    "                            self.sy, self.sx, self.ph, self.pw,\n",
    "                            gx)\n",
    "        return gx,\n",
    "\n",
    "    def create_pool_desc(self):\n",
    "        return cudnn.create_pooling_descriptor(\n",
    "            (self.kh, self.kw), (self.sy, self.sx), (self.ph, self.pw),\n",
    "            libcudnn.CUDNN_POOLING_MAX)\n",
    "\n",
    "\n",
    "def max_pooling_2d(x, ksize, stride=None, pad=0, cover_all=True,\n",
    "                   use_cudnn=True):\n",
    "    \"\"\"Spatial max pooling function.\n",
    "\n",
    "    This function acts similarly to :class:`~functions.Convolution2D`, but\n",
    "    it computes the maximum of input spatial patch for each channel\n",
    "    without any parameter instead of computing the inner products.\n",
    "\n",
    "    Args:\n",
    "        x (~chainer.Variable): Input variable.\n",
    "        ksize (int or pair of ints): Size of pooling window. ``ksize=k`` and\n",
    "            ``ksize=(k, k)`` are equivalent.\n",
    "        stride (int or pair of ints or None): Stride of pooling applications.\n",
    "            ``stride=s`` and ``stride=(s, s)`` are equivalent. If ``None`` is\n",
    "            specified, then it uses same stride as the pooling window size.\n",
    "        pad (int or pair of ints): Spatial padding width for the input array.\n",
    "            ``pad=p`` and ``pad=(p, p)`` are equivalent.\n",
    "        cover_all (bool): If ``True``, all spatial locations are pooled into\n",
    "            some output pixels. It may make the output size larger.\n",
    "        use_cudnn (bool): If ``True`` and cuDNN is enabled, then this function\n",
    "            uses cuDNN as the core implementation.\n",
    "\n",
    "    Returns:\n",
    "        ~chainer.Variable: Output variable.\n",
    "\n",
    "    \"\"\"\n",
    "    return MaxPooling2D(ksize, stride, pad, cover_all, use_cudnn)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from chainer import cuda\n",
    "from chainer.functions.pooling import pooling_2d\n",
    "from chainer.utils import conv\n",
    "from chainer.utils import type_check\n",
    "\n",
    "class Upsampling2D(pooling_2d.Pooling2D):\n",
    "    \n",
    "    \"\"\"Upsampling for SegNet.\"\"\"\n",
    "    \n",
    "    def __init__(self, indices, ksize, stride=None, pad=0,\n",
    "                 outsize=None, cover_all=True):\n",
    "        super(Upsampling2D, self).__init__(ksize, stride, pad, cover_all)\n",
    "        self.outh, self.outw = (None, None) if outsize is None else outsize\n",
    "        self.indices = indices\n",
    "        \n",
    "    def check_type_forward(self, in_types):\n",
    "        n_in = in_types.size()\n",
    "        type_check.expect(n_in == 1)\n",
    "        x_type = in_types[0]\n",
    "\n",
    "        type_check.expect(\n",
    "            x_type.dtype.kind == 'f',\n",
    "            x_type.ndim == 4,\n",
    "        )\n",
    "\n",
    "        if self.outh is not None:\n",
    "            expected_h = conv.get_conv_outsize(\n",
    "                self.outh, self.kh, self.sy, self.ph, cover_all=self.cover_all)\n",
    "            type_check.expect(x_type.shape[2] == expected_h)\n",
    "        if self.outw is not None:\n",
    "            expected_w = conv.get_conv_outsize(\n",
    "                self.outw, self.kw, self.sx, self.pw, cover_all=self.cover_all)\n",
    "            type_check.expect(x_type.shape[3] == expected_w)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x[0].shape\n",
    "        if self.outh is None:\n",
    "            self.outh = conv.get_deconv_outsize(\n",
    "                h, self.kh, self.sy, self.ph, cover_all=self.cover_all)\n",
    "        if self.outw is None:\n",
    "            self.outw = conv.get_deconv_outsize(\n",
    "                w, self.kw, self.sx, self.pw, cover_all=self.cover_all)\n",
    "        xp = cuda.get_array_module(*x)\n",
    "        col = xp.zeros((n, c, self.kh, self.kw, h, w), dtype=x[0].dtype)\n",
    "        col_r = numpy.rollaxis(col.reshape(n, c, -1, h, w), 2)\n",
    "        for i in numpy.ndindex(n, c, h, w):\n",
    "            col_r[self.indices[i]][i] = x[0][i]\n",
    "            \n",
    "        if isinstance(x[0], cuda.ndarray):\n",
    "            y = conv.col2im_gpu(col, self.sy, self.sx, self.ph, self.pw,\n",
    "                                self.outh, self.outw)\n",
    "        else:\n",
    "            y = conv.col2im_cpu(col, self.sy, self.sx, self.ph, self.pw,\n",
    "                                self.outh, self.outw)\n",
    "        return y,\n",
    "    \n",
    "    def backward(self, x, gy):\n",
    "        n, c, h, w = x[0].shape\n",
    "        xp = cuda.get_array_module(gy[0])\n",
    "        gx = xp.zeros_like(x[0])\n",
    "        if isinstance(gy[0], cuda.ndarray):\n",
    "            gcol = conv.im2col_gpu(\n",
    "                gy[0], self.kh, self.kw, self.sy, self.sx, self.ph, self.pw,\n",
    "                cover_all=self.cover_all)\n",
    "        else:\n",
    "            gcol = conv.im2col_cpu(\n",
    "                gy[0], self.kh, self.kw, self.sy, self.sx, self.ph, self.pw,\n",
    "                cover_all=self.cover_all)\n",
    "        gcol_r = numpy.rollaxis(gcol.reshape(n, c, -1, h, w), 2)\n",
    "        for i in numpy.ndindex(n, c, h, w):\n",
    "            gx[i] += gcol_r[self.indices[i]][i]\n",
    "        return gx,\n",
    "    \n",
    "def up_sampling_2d(x, indices, ksize, stride=None, pad=0, outsize=None, cover_all=True):\n",
    "\n",
    "    return Upsampling2D(indices, ksize, stride, pad, outsize, cover_all)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SegNet(chainer.Chain):\n",
    "    \n",
    "    def __init__(self):\n",
    "        NUM_CLASS = 21\n",
    "        super(SegNet, self).__init__(\n",
    "            #ENCODER\n",
    "            conv1_1=L.Convolution2D(3, 64, 3, stride=1, pad=1),\n",
    "            conv1_2=L.Convolution2D(64, 64, 3, stride=1, pad=1),\n",
    "            bn_conv1_1=L.BatchNormalization(64),\n",
    "            bn_conv1_2=L.BatchNormalization(64),\n",
    "\n",
    "            conv2_1=L.Convolution2D(64, 128, 3, stride=1, pad=1),\n",
    "            conv2_2=L.Convolution2D(128, 128, 3, stride=1, pad=1),\n",
    "            bn_conv2_1=L.BatchNormalization(128),\n",
    "            bn_conv2_2=L.BatchNormalization(128),\n",
    "\n",
    "            conv3_1=L.Convolution2D(128, 256, 3, stride=1, pad=1),\n",
    "            conv3_2=L.Convolution2D(256, 256, 3, stride=1, pad=1),\n",
    "            conv3_3=L.Convolution2D(256, 256, 3, stride=1, pad=1),\n",
    "            bn_conv3_1=L.BatchNormalization(256),\n",
    "            bn_conv3_2=L.BatchNormalization(256),\n",
    "            bn_conv3_3=L.BatchNormalization(256),           \n",
    "\n",
    "            conv4_1=L.Convolution2D(256, 512, 3, stride=1, pad=1),\n",
    "            conv4_2=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv4_3=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            bn_conv4_1=L.BatchNormalization(512),\n",
    "            bn_conv4_2=L.BatchNormalization(512),\n",
    "            bn_conv4_3=L.BatchNormalization(512), \n",
    "\n",
    "            conv5_1=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv5_2=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv5_3=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            bn_conv5_1=L.BatchNormalization(512),\n",
    "            bn_conv5_2=L.BatchNormalization(512),\n",
    "            bn_conv5_3=L.BatchNormalization(512),\n",
    "\n",
    "            #DECODER\n",
    "            conv6_1=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv6_2=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv6_3=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            bn_conv6_1=L.BatchNormalization(512),\n",
    "            bn_conv6_2=L.BatchNormalization(512),\n",
    "            bn_conv6_3=L.BatchNormalization(512),\n",
    "            \n",
    "            conv7_1=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv7_2=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv7_3=L.Convolution2D(512, 256, 3, stride=1, pad=1),\n",
    "            bn_conv7_1=L.BatchNormalization(512),\n",
    "            bn_conv7_2=L.BatchNormalization(512),\n",
    "            bn_conv7_3=L.BatchNormalization(256), \n",
    "\n",
    "            conv8_1=L.Convolution2D(256, 256, 3, stride=1, pad=1),\n",
    "            conv8_2=L.Convolution2D(256, 256, 3, stride=1, pad=1),\n",
    "            conv8_3=L.Convolution2D(256, 128, 3, stride=1, pad=1),\n",
    "            bn_conv8_1=L.BatchNormalization(256),\n",
    "            bn_conv8_2=L.BatchNormalization(256),\n",
    "            bn_conv8_3=L.BatchNormalization(128),\n",
    "\n",
    "            conv9_1=L.Convolution2D(128, 128, 3, stride=1, pad=1),\n",
    "            conv9_2=L.Convolution2D(128, 64, 3, stride=1, pad=1),\n",
    "            bn_conv9_1=L.BatchNormalization(128),\n",
    "            bn_conv9_2=L.BatchNormalization(64),\n",
    "            \n",
    "            conv10_1=L.Convolution2D(64, 64, 3, stride=1, pad=1),\n",
    "            conv10_2=L.Convolution2D(64, NUM_CLASS, 3, stride=1, pad=1),\n",
    "            bn_conv10_1=L.BatchNormalization(64),\n",
    "            bn_conv10_2=L.BatchNormalization(NUM_CLASS),        \n",
    "        )\n",
    "        self.train = True\n",
    "        self.train_decoder_only = True\n",
    "\n",
    "    def __call__(self, x, t=None):\n",
    "        x.volatile = False if not self.train_decoder_only and self.train else True\n",
    "        #ENCODING\n",
    "        h = F.relu(self.bn_conv1_1(self.conv1_1(x), test=not self.train))\n",
    "        h = F.relu(self.bn_conv1_2(self.conv1_2(h), test=not self.train))\n",
    "        h, ind1, size1 = max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.bn_conv2_1(self.conv2_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv2_2(self.conv2_2(h), test=not self.train))\n",
    "        h, ind2, size2 = max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.bn_conv3_1(self.conv3_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv3_2(self.conv3_2(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv3_3(self.conv3_3(h), test=not self.train))\n",
    "        h, ind3, size3 = max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.bn_conv4_1(self.conv4_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv4_2(self.conv4_2(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv4_3(self.conv4_3(h), test=not self.train))\n",
    "        h, ind4, size4 = max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.bn_conv5_1(self.conv5_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv5_2(self.conv5_2(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv5_3(self.conv5_3(h), test=not self.train))\n",
    "        h, ind5, size5 = max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        #DECODING\n",
    "        h.volatile = not self.train\n",
    "        \n",
    "        h = up_sampling_2d(h, indices=ind5.data, ksize=2, stride=2, outsize=size5.data)\n",
    "        h = F.relu(self.bn_conv6_1(self.conv6_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv6_2(self.conv6_2(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv6_3(self.conv6_3(h), test=not self.train))\n",
    "        \n",
    "        h = up_sampling_2d(h, indices=ind4.data, ksize=2, stride=2, outsize=size4.data)\n",
    "        h = F.relu(self.bn_conv7_1(self.conv7_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv7_2(self.conv7_2(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv7_3(self.conv7_3(h), test=not self.train))\n",
    "        \n",
    "        h = up_sampling_2d(h, indices=ind3.data, ksize=2, stride=2, outsize=size3.data)\n",
    "        h = F.relu(self.bn_conv8_1(self.conv8_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv8_2(self.conv8_2(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv8_3(self.conv8_3(h), test=not self.train))\n",
    "        \n",
    "        h = up_sampling_2d(h, indices=ind2.data, ksize=2, stride=2, outsize=size2.data)\n",
    "        h = F.relu(self.bn_conv9_1(self.conv9_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv9_2(self.conv9_2(h), test=not self.train))\n",
    "        \n",
    "        h = up_sampling_2d(h, indices=ind1.data, ksize=2, stride=2, outsize=size1.data)\n",
    "        h = F.relu(self.bn_conv10_1(self.conv10_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv10_2(self.conv10_2(h), test=not self.train))\n",
    "        \n",
    "        if self.train:\n",
    "            return F.softmax_cross_entropy(h, t)\n",
    "        else:\n",
    "            return F.softmax(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_model(src, dst):\n",
    "    assert isinstance(src, chainer.link.Chain)\n",
    "    assert isinstance(dst, chainer.link.Chain)\n",
    "    for child in src.children():\n",
    "        if child.name not in dst.__dict__: continue\n",
    "        dst_child = dst[child.name]\n",
    "        if type(child) != type(dst_child): continue\n",
    "        if isinstance(child, chainer.link.Chain):\n",
    "            copy_model(child, dst_child)\n",
    "        if isinstance(child, chainer.link.Link):\n",
    "            match = True\n",
    "            for a, b in zip(child.namedparams(), dst_child.namedparams()):\n",
    "                if a[0] != b[0]:\n",
    "                    match = False\n",
    "                    break\n",
    "                if a[1].data.shape != b[1].data.shape:\n",
    "                    match = False\n",
    "                    break\n",
    "            if not match:\n",
    "                print 'Ignore %s because of parameter mismatch' % child.name\n",
    "                continue\n",
    "            for a, b in zip(child.namedparams(), dst_child.namedparams()):\n",
    "                b[1].data = a[1].data\n",
    "            print 'Copy %s' % child.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segnet=SegNet()\n",
    "segnet.train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Variable(numpy.random.random((1, 3, 32, 32)).astype('f'))\n",
    "t = Variable(numpy.zeros((1, 32, 32), dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segnet.zerograds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = segnet(x, t)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [  0.00000000e+00,  -1.80259242e-03,   1.38448074e-03],\n",
       "         [  0.00000000e+00,  -5.78853011e-04,   9.96965100e-04]],\n",
       "\n",
       "        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [ -1.21384207e-03,   9.32291208e-04,   0.00000000e+00],\n",
       "         [ -3.89792054e-04,   6.71343238e-04,   0.00000000e+00]],\n",
       "\n",
       "        [[ -1.50395592e-03,   1.15511310e-03,   0.00000000e+00],\n",
       "         [ -4.82954114e-04,   8.31797312e-04,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "        ..., \n",
       "        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [ -1.97727676e-03,   1.51864707e-03,   0.00000000e+00],\n",
       "         [ -6.34948141e-04,   1.09357829e-03,   0.00000000e+00]],\n",
       "\n",
       "        [[  0.00000000e+00,  -1.80244690e-03,   1.38436898e-03],\n",
       "         [  0.00000000e+00,  -5.78806328e-04,   9.96884657e-04],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "        [[ -1.58629357e-03,   1.21835247e-03,   0.00000000e+00],\n",
       "         [ -5.09394566e-04,   8.77336075e-04,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   1.04864783e-04,  -4.02253972e-05],\n",
       "         [  0.00000000e+00,  -5.42306043e-05,  -1.04089268e-05]],\n",
       "\n",
       "        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [  7.06145656e-05,  -2.70872533e-05,   0.00000000e+00],\n",
       "         [ -3.65181804e-05,  -7.00923511e-06,   0.00000000e+00]],\n",
       "\n",
       "        [[  8.74917750e-05,  -3.35612312e-05,   0.00000000e+00],\n",
       "         [ -4.52461900e-05,  -8.68447387e-06,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "        ..., \n",
       "        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [  1.15026945e-04,  -4.41235316e-05,   0.00000000e+00],\n",
       "         [ -5.94859484e-05,  -1.14176282e-05,   0.00000000e+00]],\n",
       "\n",
       "        [[  0.00000000e+00,   1.04856314e-04,  -4.02221485e-05],\n",
       "         [  0.00000000e+00,  -5.42262278e-05,  -1.04080864e-05],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "        [[  9.22817198e-05,  -3.53986215e-05,   0.00000000e+00],\n",
       "         [ -4.77233007e-05,  -9.15992587e-06,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [  0.00000000e+00,  -7.49611631e-02,   8.55779555e-03],\n",
       "         [  0.00000000e+00,   1.19081056e-02,   5.44952601e-02]],\n",
       "\n",
       "        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [ -5.04778624e-02,   5.76270744e-03,   0.00000000e+00],\n",
       "         [  8.01876187e-03,   3.66963930e-02,   0.00000000e+00]],\n",
       "\n",
       "        [[ -6.25423044e-02,   7.14002037e-03,   0.00000000e+00],\n",
       "         [  9.93528310e-03,   4.54670005e-02,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "        ..., \n",
       "        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [ -8.22254494e-02,   9.38710850e-03,   0.00000000e+00],\n",
       "         [  1.30620878e-02,   5.97762503e-02,   0.00000000e+00]],\n",
       "\n",
       "        [[  0.00000000e+00,  -7.49551132e-02,   8.55710451e-03],\n",
       "         [  0.00000000e+00,   1.19071435e-02,   5.44908606e-02],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "        [[ -6.59663305e-02,   7.53091788e-03,   0.00000000e+00],\n",
       "         [  1.04792137e-02,   4.79561985e-02,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   1.14352129e-04,   2.89633346e-04],\n",
       "         [  0.00000000e+00,   9.78939468e-04,  -1.38292485e-03]],\n",
       "\n",
       "        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [  7.70032275e-05,   1.95035303e-04,   0.00000000e+00],\n",
       "         [  6.59204961e-04,  -9.31243470e-04,   0.00000000e+00]],\n",
       "\n",
       "        [[  9.54073475e-05,   2.41649643e-04,   0.00000000e+00],\n",
       "         [  8.16757965e-04,  -1.15381496e-03,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "        ..., \n",
       "        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [  1.25433697e-04,   3.17700964e-04,   0.00000000e+00],\n",
       "         [  1.07380585e-03,  -1.51694042e-03,   0.00000000e+00]],\n",
       "\n",
       "        [[  0.00000000e+00,   1.14342896e-04,   2.89609976e-04],\n",
       "         [  0.00000000e+00,   9.78860422e-04,  -1.38281321e-03],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "        [[  1.00630648e-04,   2.54879327e-04,   0.00000000e+00],\n",
       "         [  8.61473323e-04,  -1.21698319e-03,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   3.77433840e-04,   3.87261418e-04],\n",
       "         [  0.00000000e+00,  -1.45221350e-03,   6.87518215e-04]],\n",
       "\n",
       "        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [  2.54159007e-04,   2.60776753e-04,   0.00000000e+00],\n",
       "         [ -9.77901509e-04,   4.62965778e-04,   0.00000000e+00]],\n",
       "\n",
       "        [[  3.14904173e-04,   3.23103595e-04,   0.00000000e+00],\n",
       "         [ -1.21162448e-03,   5.73616708e-04,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "        ..., \n",
       "        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [  4.14009963e-04,   4.24789905e-04,   0.00000000e+00],\n",
       "         [ -1.59294356e-03,   7.54143810e-04,   0.00000000e+00]],\n",
       "\n",
       "        [[  0.00000000e+00,   3.77403368e-04,   3.87230160e-04],\n",
       "         [  0.00000000e+00,  -1.45209627e-03,   6.87462743e-04],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "        [[  3.32144351e-04,   3.40792671e-04,   0.00000000e+00],\n",
       "         [ -1.27795769e-03,   6.05020730e-04,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [  0.00000000e+00,  -1.34449266e-02,   4.00433876e-03],\n",
       "         [  0.00000000e+00,   2.11234554e-03,   7.32824253e-03]],\n",
       "\n",
       "        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [ -9.05363727e-03,   2.69646943e-03,   0.00000000e+00],\n",
       "         [  1.42242573e-03,   4.93474258e-03,   0.00000000e+00]],\n",
       "\n",
       "        [[ -1.12174982e-02,   3.34093790e-03,   0.00000000e+00],\n",
       "         [  1.76239200e-03,   6.11416856e-03,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "        ..., \n",
       "        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "         [ -1.47478394e-02,   4.39238874e-03,   0.00000000e+00],\n",
       "         [  2.31704721e-03,   8.03840347e-03,   0.00000000e+00]],\n",
       "\n",
       "        [[  0.00000000e+00,  -1.34438416e-02,   4.00401559e-03],\n",
       "         [  0.00000000e+00,   2.11217511e-03,   7.32765114e-03],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "        [[ -1.18316272e-02,   3.52384546e-03,   0.00000000e+00],\n",
       "         [  1.85887842e-03,   6.44890312e-03,   0.00000000e+00],\n",
       "         [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00]]]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segnet.conv6_1.W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
