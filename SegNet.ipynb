{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import cuda\n",
    "from chainer import Variable\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from chainer import cuda\n",
    "from chainer.functions.pooling import pooling_2d\n",
    "from chainer.utils import conv\n",
    "\n",
    "if cuda.cudnn_enabled:\n",
    "    cudnn = cuda.cudnn\n",
    "    libcudnn = cudnn.cudnn\n",
    "\n",
    "\n",
    "class MaxPooling2D(pooling_2d.Pooling2D):\n",
    "\n",
    "    \"\"\"Max pooling over a set of 2d planes.\"\"\"\n",
    "\n",
    "    def forward_cpu(self, x):\n",
    "        n, c, h, w = x[0].shape\n",
    "        col = conv.im2col_cpu(\n",
    "            x[0], self.kh, self.kw, self.sy, self.sx, self.ph, self.pw,\n",
    "            pval=-float('inf'), cover_all=self.cover_all)\n",
    "        n, c, kh, kw, out_h, out_w = col.shape\n",
    "        col = col.reshape(n, c, kh * kw, out_h, out_w)\n",
    "\n",
    "        # We select maximum twice, since the implementation using numpy.choose\n",
    "        # hits its bug when kh * kw >= 32.\n",
    "        self.indexes = col.argmax(axis=2)\n",
    "        y = col.max(axis=2)\n",
    "        return y, self.indexes, np.array([h, w])\n",
    "\n",
    "    def forward_gpu(self, x):\n",
    "        if (cuda.cudnn_enabled and self.use_cudnn and\n",
    "                pooling_2d._check_cudnn_acceptable_type(x[0].dtype)):\n",
    "            return super(MaxPooling2D, self).forward_gpu(x)\n",
    "\n",
    "        n, c, h, w = x[0].shape\n",
    "        y_h = conv.get_conv_outsize(\n",
    "            h, self.kh, self.sy, self.ph, self.cover_all)\n",
    "        y_w = conv.get_conv_outsize(\n",
    "            w, self.kw, self.sx, self.pw, self.cover_all)\n",
    "        y = cuda.cupy.empty((n, c, y_h, y_w), dtype=x[0].dtype)\n",
    "        self.indexes = cuda.cupy.empty((n, c, y_h, y_w), dtype=numpy.int32)\n",
    "\n",
    "        cuda.elementwise(\n",
    "            'raw T in, int32 h, int32 w, int32 out_h, int32 out_w,'\n",
    "            'int32 kh, int32 kw, int32 sy, int32 sx, int32 ph, int32 pw',\n",
    "            'T out, S indexes',\n",
    "            '''\n",
    "               int c0    = i / (out_h * out_w);\n",
    "               int out_y = i / out_w % out_h;\n",
    "               int out_x = i % out_w;\n",
    "               int in_y_0 = max(0, out_y * sy - ph);\n",
    "               int in_y_1 = min(h, out_y * sy + kh - ph);\n",
    "               int in_x_0 = max(0, out_x * sx - pw);\n",
    "               int in_x_1 = min(w, out_x * sx + kw - pw);\n",
    "\n",
    "               T maxval = in[in_x_0 + w * (in_y_0 + h * c0)];\n",
    "               int argmax_y = in_y_0;\n",
    "               int argmax_x = in_x_0;\n",
    "               for (int y = in_y_0; y < in_y_1; ++y) {\n",
    "                 int offset_y = w * (y + h * c0);\n",
    "                 for (int x = in_x_0; x < in_x_1; ++x) {\n",
    "                   float v = in[x + offset_y];\n",
    "                   if (maxval < v) {\n",
    "                     maxval   = v;\n",
    "                     argmax_y = y;\n",
    "                     argmax_x = x;\n",
    "                   }\n",
    "                 }\n",
    "               }\n",
    "               out = maxval;\n",
    "\n",
    "               int argmax_ky = argmax_y + ph - out_y * sy;\n",
    "               int argmax_kx = argmax_x + pw - out_x * sx;\n",
    "               indexes = argmax_kx + kw * argmax_ky;\n",
    "            ''', 'max_pool_fwd')(x[0].reduced_view(),\n",
    "                                 h, w, y_h, y_w, self.kh, self.kw,\n",
    "                                 self.sy, self.sx, self.ph, self.pw,\n",
    "                                 y, self.indexes)\n",
    "        return y, self.indexes, np.array([h, w]),\n",
    "\n",
    "    def backward_cpu(self, x, gy):\n",
    "        n, c, out_h, out_w = gy[0].shape\n",
    "        h, w = x[0].shape[2:]\n",
    "        gcol = numpy.zeros(\n",
    "            (n, c, self.kh, self.kw, out_h, out_w), dtype=x[0].dtype)\n",
    "\n",
    "        # TODO(beam2d): Make it fast\n",
    "        gcol_r = numpy.rollaxis(gcol.reshape(n, c, -1, out_h, out_w), 2)\n",
    "        for i in numpy.ndindex(n, c, out_h, out_w):\n",
    "            gcol_r[self.indexes[i]][i] = gy[0][i]\n",
    "\n",
    "        gx = conv.col2im_cpu(gcol, self.sy, self.sx, self.ph, self.pw, h, w)\n",
    "        return gx,\n",
    "\n",
    "    def backward_gpu(self, x, gy):\n",
    "        if (cuda.cudnn_enabled and self.use_cudnn and\n",
    "                pooling_2d._check_cudnn_acceptable_type(x[0].dtype)):\n",
    "            return super(MaxPooling2D, self).backward_gpu(x, gy)\n",
    "\n",
    "        n, c, h, w = x[0].shape\n",
    "        y_h, y_w = gy[0].shape[2:]\n",
    "        gx = cuda.cupy.empty_like(x[0])\n",
    "\n",
    "        cuda.elementwise(\n",
    "            'raw T gy, raw S indexes, int32 h, int32 w,'\n",
    "            'int32 out_h, int32 out_w, int32 kh, int32 kw,'\n",
    "            'int32 sy, int32 sx, int32 ph, int32 pw',\n",
    "            'T gx',\n",
    "            '''\n",
    "               int c0 = i / (h * w);\n",
    "               int y  = i / w % h + ph;\n",
    "               int x  = i % w + pw;\n",
    "               int out_y_0 = max(0,     (y - kh + sy) / sy);\n",
    "               int out_y_1 = min(out_h, (y      + sy) / sy);\n",
    "               int out_x_0 = max(0,     (x - kw + sx) / sx);\n",
    "               int out_x_1 = min(out_w, (x      + sx) / sx);\n",
    "\n",
    "               T val = 0;\n",
    "               for (int out_y = out_y_0; out_y < out_y_1; ++out_y) {\n",
    "                 int ky = y - out_y * sy;\n",
    "                 for (int out_x = out_x_0; out_x < out_x_1; ++out_x) {\n",
    "                   int kx = x - out_x * sx;\n",
    "                   int offset = out_x + out_w * (out_y + out_h * c0);\n",
    "                   if (indexes[offset] == kx + kw * ky) {\n",
    "                     val = val + gy[offset];\n",
    "                   }\n",
    "                 }\n",
    "               }\n",
    "               gx = val;\n",
    "            ''',\n",
    "            'max_pool_bwd')(gy[0].reduced_view(), self.indexes.reduced_view(),\n",
    "                            h, w, y_h, y_w, self.kh, self.kw,\n",
    "                            self.sy, self.sx, self.ph, self.pw,\n",
    "                            gx)\n",
    "        return gx,\n",
    "\n",
    "    def create_pool_desc(self):\n",
    "        return cudnn.create_pooling_descriptor(\n",
    "            (self.kh, self.kw), (self.sy, self.sx), (self.ph, self.pw),\n",
    "            libcudnn.CUDNN_POOLING_MAX)\n",
    "\n",
    "\n",
    "def max_pooling_2d(x, ksize, stride=None, pad=0, cover_all=True,\n",
    "                   use_cudnn=True):\n",
    "    \"\"\"Spatial max pooling function.\n",
    "\n",
    "    This function acts similarly to :class:`~functions.Convolution2D`, but\n",
    "    it computes the maximum of input spatial patch for each channel\n",
    "    without any parameter instead of computing the inner products.\n",
    "\n",
    "    Args:\n",
    "        x (~chainer.Variable): Input variable.\n",
    "        ksize (int or pair of ints): Size of pooling window. ``ksize=k`` and\n",
    "            ``ksize=(k, k)`` are equivalent.\n",
    "        stride (int or pair of ints or None): Stride of pooling applications.\n",
    "            ``stride=s`` and ``stride=(s, s)`` are equivalent. If ``None`` is\n",
    "            specified, then it uses same stride as the pooling window size.\n",
    "        pad (int or pair of ints): Spatial padding width for the input array.\n",
    "            ``pad=p`` and ``pad=(p, p)`` are equivalent.\n",
    "        cover_all (bool): If ``True``, all spatial locations are pooled into\n",
    "            some output pixels. It may make the output size larger.\n",
    "        use_cudnn (bool): If ``True`` and cuDNN is enabled, then this function\n",
    "            uses cuDNN as the core implementation.\n",
    "\n",
    "    Returns:\n",
    "        ~chainer.Variable: Output variable.\n",
    "\n",
    "    \"\"\"\n",
    "    return MaxPooling2D(ksize, stride, pad, cover_all, use_cudnn)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from chainer import cuda\n",
    "from chainer.functions.pooling import pooling_2d\n",
    "from chainer.utils import conv\n",
    "from chainer.utils import type_check\n",
    "\n",
    "class Upsampling2D(pooling_2d.Pooling2D):\n",
    "    \n",
    "    \"\"\"Upsampling for SegNet.\"\"\"\n",
    "    \n",
    "    def __init__(self, indices, ksize, stride=None, pad=0,\n",
    "                 outsize=None, cover_all=True):\n",
    "        super(Upsampling2D, self).__init__(ksize, stride, pad, cover_all)\n",
    "        self.outh, self.outw = (None, None) if outsize is None else outsize\n",
    "        self.indices = indices\n",
    "        \n",
    "    def check_type_forward(self, in_types):\n",
    "        n_in = in_types.size()\n",
    "        type_check.expect(n_in == 1)\n",
    "        x_type = in_types[0]\n",
    "\n",
    "        type_check.expect(\n",
    "            x_type.dtype.kind == 'f',\n",
    "            x_type.ndim == 4,\n",
    "        )\n",
    "\n",
    "        if self.outh is not None:\n",
    "            expected_h = conv.get_conv_outsize(\n",
    "                self.outh, self.kh, self.sy, self.ph, cover_all=self.cover_all)\n",
    "            type_check.expect(x_type.shape[2] == expected_h)\n",
    "        if self.outw is not None:\n",
    "            expected_w = conv.get_conv_outsize(\n",
    "                self.outw, self.kw, self.sx, self.pw, cover_all=self.cover_all)\n",
    "            type_check.expect(x_type.shape[3] == expected_w)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x[0].shape\n",
    "        if self.outh is None:\n",
    "            self.outh = conv.get_deconv_outsize(\n",
    "                h, self.kh, self.sy, self.ph, cover_all=self.cover_all)\n",
    "        if self.outw is None:\n",
    "            self.outw = conv.get_deconv_outsize(\n",
    "                w, self.kw, self.sx, self.pw, cover_all=self.cover_all)\n",
    "        xp = cuda.get_array_module(*x)\n",
    "        col = xp.zeros((n, c, self.kh, self.kw, h, w), dtype=x[0].dtype)\n",
    "        col_r = numpy.rollaxis(col.reshape(n, c, -1, h, w), 2)\n",
    "        for i in numpy.ndindex(n, c, h, w):\n",
    "            col_r[self.indices[i]][i] = x[0][i]\n",
    "            \n",
    "        if isinstance(x[0], cuda.ndarray):\n",
    "            y = conv.col2im_gpu(col, self.sy, self.sx, self.ph, self.pw,\n",
    "                                self.outh, self.outw)\n",
    "        else:\n",
    "            y = conv.col2im_cpu(col, self.sy, self.sx, self.ph, self.pw,\n",
    "                                self.outh, self.outw)\n",
    "        return y,\n",
    "    \n",
    "    def backward(self, x, gy):\n",
    "        n, c, h, w = x[0].shape\n",
    "        xp = cuda.get_array_module(gy[0])\n",
    "        gx = xp.zeros_like(x[0])\n",
    "        if isinstance(gy[0], cuda.ndarray):\n",
    "            gcol = conv.im2col_gpu(\n",
    "                gy[0], self.kh, self.kw, self.sy, self.sx, self.ph, self.pw,\n",
    "                cover_all=self.cover_all)\n",
    "        else:\n",
    "            gcol = conv.im2col_cpu(\n",
    "                gy[0], self.kh, self.kw, self.sy, self.sx, self.ph, self.pw,\n",
    "                cover_all=self.cover_all)\n",
    "        gcol_r = numpy.rollaxis(gcol.reshape(n, c, -1, h, w), 2)\n",
    "        for i in numpy.ndindex(n, c, h, w):\n",
    "            gx[i] += gcol_r[self.indices[i]][i]\n",
    "        return gx,\n",
    "    \n",
    "def up_sampling_2d(x, indices, ksize, stride=None, pad=0, outsize=None, cover_all=True):\n",
    "\n",
    "    return Upsampling2D(indices, ksize, stride, pad, outsize, cover_all)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SegNet(chainer.Chain):\n",
    "    \n",
    "    def __init__(self):\n",
    "        NUM_CLASS = 21\n",
    "        super(SegNet, self).__init__(\n",
    "            #ENCODER\n",
    "            conv1_1=L.Convolution2D(3, 64, 3, stride=1, pad=1),\n",
    "            conv1_2=L.Convolution2D(64, 64, 3, stride=1, pad=1),\n",
    "            bn_conv1_1=L.BatchNormalization(64),\n",
    "            bn_conv1_2=L.BatchNormalization(64),\n",
    "\n",
    "            conv2_1=L.Convolution2D(64, 128, 3, stride=1, pad=1),\n",
    "            conv2_2=L.Convolution2D(128, 128, 3, stride=1, pad=1),\n",
    "            bn_conv2_1=L.BatchNormalization(128),\n",
    "            bn_conv2_2=L.BatchNormalization(128),\n",
    "\n",
    "            conv3_1=L.Convolution2D(128, 256, 3, stride=1, pad=1),\n",
    "            conv3_2=L.Convolution2D(256, 256, 3, stride=1, pad=1),\n",
    "            conv3_3=L.Convolution2D(256, 256, 3, stride=1, pad=1),\n",
    "            bn_conv3_1=L.BatchNormalization(256),\n",
    "            bn_conv3_2=L.BatchNormalization(256),\n",
    "            bn_conv3_3=L.BatchNormalization(256),           \n",
    "\n",
    "            conv4_1=L.Convolution2D(256, 512, 3, stride=1, pad=1),\n",
    "            conv4_2=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv4_3=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            bn_conv4_1=L.BatchNormalization(512),\n",
    "            bn_conv4_2=L.BatchNormalization(512),\n",
    "            bn_conv4_3=L.BatchNormalization(512), \n",
    "\n",
    "            conv5_1=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv5_2=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv5_3=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            bn_conv5_1=L.BatchNormalization(512),\n",
    "            bn_conv5_2=L.BatchNormalization(512),\n",
    "            bn_conv5_3=L.BatchNormalization(512),\n",
    "\n",
    "            #DECODER\n",
    "            conv6_1=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv6_2=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv6_3=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            bn_conv6_1=L.BatchNormalization(512),\n",
    "            bn_conv6_2=L.BatchNormalization(512),\n",
    "            bn_conv6_3=L.BatchNormalization(512),\n",
    "            \n",
    "            conv7_1=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv7_2=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv7_3=L.Convolution2D(512, 256, 3, stride=1, pad=1),\n",
    "            bn_conv7_1=L.BatchNormalization(512),\n",
    "            bn_conv7_2=L.BatchNormalization(512),\n",
    "            bn_conv7_3=L.BatchNormalization(256), \n",
    "\n",
    "            conv8_1=L.Convolution2D(256, 256, 3, stride=1, pad=1),\n",
    "            conv8_2=L.Convolution2D(256, 256, 3, stride=1, pad=1),\n",
    "            conv8_3=L.Convolution2D(256, 128, 3, stride=1, pad=1),\n",
    "            bn_conv8_1=L.BatchNormalization(256),\n",
    "            bn_conv8_2=L.BatchNormalization(256),\n",
    "            bn_conv8_3=L.BatchNormalization(128),\n",
    "\n",
    "            conv9_1=L.Convolution2D(128, 128, 3, stride=1, pad=1),\n",
    "            conv9_2=L.Convolution2D(128, 64, 3, stride=1, pad=1),\n",
    "            bn_conv9_1=L.BatchNormalization(128),\n",
    "            bn_conv9_2=L.BatchNormalization(64),\n",
    "            \n",
    "            conv10_1=L.Convolution2D(64, 64, 3, stride=1, pad=1),\n",
    "            conv10_2=L.Convolution2D(64, NUM_CLASS, 3, stride=1, pad=1),\n",
    "            bn_conv10_1=L.BatchNormalization(64),\n",
    "            bn_conv10_2=L.BatchNormalization(NUM_CLASS),        \n",
    "        )\n",
    "        self.train = True\n",
    "\n",
    "    def __call__(self, x, t=None):\n",
    "        \n",
    "        #ENCODING\n",
    "        h = F.relu(self.bn_conv1_1(self.conv1_1(x), test=not self.train))\n",
    "        h = F.relu(self.bn_conv1_2(self.conv1_2(h), test=not self.train))\n",
    "        h, ind1, size1 = max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.bn_conv2_1(self.conv2_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv2_2(self.conv2_2(h), test=not self.train))\n",
    "        h, ind2, size2 = max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.bn_conv3_1(self.conv3_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv3_2(self.conv3_2(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv3_3(self.conv3_3(h), test=not self.train))\n",
    "        h, ind3, size3 = max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.bn_conv4_1(self.conv4_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv4_2(self.conv4_2(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv4_3(self.conv4_3(h), test=not self.train))\n",
    "        h, ind4, size4 = max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.bn_conv5_1(self.conv5_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv5_2(self.conv5_2(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv5_3(self.conv5_3(h), test=not self.train))\n",
    "        h, ind5, size5 = max_pooling_2d(h, 2, stride=2)\n",
    "        \n",
    "        #DECODING\n",
    "        h = up_sampling_2d(h, indices=ind5.data, ksize=2, stride=2, outsize=size5.data)\n",
    "        h = F.relu(self.bn_conv6_1(self.conv6_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv6_2(self.conv6_2(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv6_3(self.conv6_3(h), test=not self.train))\n",
    "        \n",
    "        h = up_sampling_2d(h, indices=ind4.data, ksize=2, stride=2, outsize=size4.data)\n",
    "        h = F.relu(self.bn_conv7_1(self.conv7_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv7_2(self.conv7_2(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv7_3(self.conv7_3(h), test=not self.train))\n",
    "        \n",
    "        h = up_sampling_2d(h, indices=ind3.data, ksize=2, stride=2, outsize=size3.data)\n",
    "        h = F.relu(self.bn_conv8_1(self.conv8_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv8_2(self.conv8_2(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv8_3(self.conv8_3(h), test=not self.train))\n",
    "        \n",
    "        h = up_sampling_2d(h, indices=ind2.data, ksize=2, stride=2, outsize=size2.data)\n",
    "        h = F.relu(self.bn_conv9_1(self.conv9_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv9_2(self.conv9_2(h), test=not self.train))\n",
    "        \n",
    "        h = up_sampling_2d(h, indices=ind1.data, ksize=2, stride=2, outsize=size1.data)\n",
    "        h = F.relu(self.bn_conv10_1(self.conv10_1(h), test=not self.train))\n",
    "        h = F.relu(self.bn_conv10_2(self.conv10_2(h), test=not self.train))\n",
    "        \n",
    "        if self.train:\n",
    "            return F.softmax_cross_entropy(h, t)\n",
    "        else:\n",
    "            return F.softmax(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_model(src, dst):\n",
    "    assert isinstance(src, chainer.link.Chain)\n",
    "    assert isinstance(dst, chainer.link.Chain)\n",
    "    for child in src.children():\n",
    "        if child.name not in dst.__dict__: continue\n",
    "        dst_child = dst[child.name]\n",
    "        if type(child) != type(dst_child): continue\n",
    "        if isinstance(child, chainer.link.Chain):\n",
    "            copy_model(child, dst_child)\n",
    "        if isinstance(child, chainer.link.Link):\n",
    "            match = True\n",
    "            for a, b in zip(child.namedparams(), dst_child.namedparams()):\n",
    "                if a[0] != b[0]:\n",
    "                    match = False\n",
    "                    break\n",
    "                if a[1].data.shape != b[1].data.shape:\n",
    "                    match = False\n",
    "                    break\n",
    "            if not match:\n",
    "                print 'Ignore %s because of parameter mismatch' % child.name\n",
    "                continue\n",
    "            for a, b in zip(child.namedparams(), dst_child.namedparams()):\n",
    "                b[1].data = a[1].data\n",
    "            print 'Copy %s' % child.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segnet.train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = segnet(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Variable(numpy.random.random((1, 3, 256, 256)).astype('f'))\n",
    "t = Variable(numpy.zeros((1, 256, 256), dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
